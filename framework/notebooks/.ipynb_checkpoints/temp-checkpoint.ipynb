{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c2d3501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd396876dcd4415b2e080097874c009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.89k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98b03e47db14dd5b0a4f1f69f56cfab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/921 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset rotten_tomatoes_movie_review/default (download: 476.34 KiB, generated: 1.28 MiB, post-processed: Unknown size, total: 1.75 MiB) to /home/alta/Conversational/OET/al826/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638ead233b424f338c35cf6b2723e383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/488k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset rotten_tomatoes_movie_review downloaded and prepared to /home/alta/Conversational/OET/al826/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aae5b85355a433891ca24a7c32e5e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2118d2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .', 'effective but too-tepid biopic', 'if you sometimes like to go to the movies to have fun , wasabi is a good place to start .', \"emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\"], 'label': [1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1624d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 8530\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def _load_rotten_tomatoes(lim:int=None):\n",
    "    dataset = load_dataset(\"rotten_tomatoes\")\n",
    "    train = list(dataset['train'])[:lim]\n",
    "    dev   = list(dataset['validation'])[:lim]\n",
    "    test  = list(dataset['test'])[:lim]\n",
    "    return train, dev, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1846b7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92e8ae60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 7592, 103, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "print(tokenizer('hello [MASK]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "740e6bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module src.trainer in src:\n",
      "\n",
      "NAME\n",
      "    src.trainer\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        Trainer\n",
      "    \n",
      "    class Trainer(builtins.object)\n",
      "     |  Trainer(exp_name: str, m_args: <function namedtuple at 0x7fa7ec2e5280>)\n",
      "     |  \n",
      "     |  \"base class for running basic transformer classification models\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, exp_name: str, m_args: <function namedtuple at 0x7fa7ec2e5280>)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  load_model(self, name: str = 'base')\n",
      "     |  \n",
      "     |  model_output(self, batch)\n",
      "     |  \n",
      "     |  save_model(self, name: str = 'base')\n",
      "     |  \n",
      "     |  set_up_helpers(self, m_args: <function namedtuple at 0x7fa7ec2e5280>)\n",
      "     |  \n",
      "     |  set_up_wandb(self, args: <function namedtuple at 0x7fa7ec2e5280>)\n",
      "     |  \n",
      "     |  system_eval = inner(*args, **kwargs)\n",
      "     |  \n",
      "     |  to(self, device)\n",
      "     |  \n",
      "     |  train(self, t_args: <function namedtuple at 0x7fa7ec2e5280>)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "DATA\n",
      "    List = typing.List\n",
      "        A generic version of list.\n",
      "    \n",
      "    Tuple = typing.Tuple\n",
      "        Tuple type; Tuple[X, Y] is the cross-product type of X and Y.\n",
      "        \n",
      "        Example: Tuple[T1, T2] is a tuple of two elements corresponding\n",
      "        to type variables T1 and T2.  Tuple[int, float, str] is a tuple\n",
      "        of an int, a float and a string.\n",
      "        \n",
      "        To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].\n",
      "\n",
      "FILE\n",
      "    /home/alta/Conversational/OET/al826/2022/shortcuts/new_system/src/trainer.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "235c218a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6ea682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.utils.data_utils import load_data\n",
    "from src.utils.torch_utils import load_tokenizer\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, trans_name):\n",
    "        self.tokenizer = load_tokenizer(trans_name)\n",
    "    \n",
    "    def prep_split(self, data:list):\n",
    "        output = []\n",
    "        for ex in tqdm(data):\n",
    "            text  = ex['text']\n",
    "            label = ex['label']\n",
    "            ids   = self.tokenizer(text).input_ids\n",
    "            output.append(SimpleNamespace(text=text, ids=ids, label=label))\n",
    "        return output\n",
    "    \n",
    "    def get_data(self, data_name:str):\n",
    "        print('tokenizing data')\n",
    "        train, dev, test = load_data(data_name)\n",
    "        train = self.prep_split(train)\n",
    "        dev   = self.prep_split(dev)\n",
    "        test  = self.prep_split(test)\n",
    "        return train, dev, test\n",
    "    \n",
    "D = DataLoader('bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5acde0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/alta/Conversational/OET/al826/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c5abaddc4b44349469d012e7fb9f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                        | 0/20000 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (727 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:25<00:00, 792.12it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:06<00:00, 819.47it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 25000/25000 [00:28<00:00, 874.41it/s]\n"
     ]
    }
   ],
   "source": [
    "train, dev, test = D.get_data('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945d8d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddded3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a0beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from tqdm import tqdm \n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Tuple\n",
    "from datasets import load_dataset\n",
    "\n",
    "def load_data(data_name:str)->Tuple['train', 'dev', 'test']:\n",
    "    if data_name == 'imdb':    return _load_imdb()\n",
    "    if data_name == 'dbpedia': return _load_dbpedia()\n",
    "    else: raise ValueError('invalid dataset provided')\n",
    "\n",
    "def _load_imdb()->List[Dict['text', 'label']]:\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "    train_data = list(dataset['train'])\n",
    "    train, dev = _create_splits(train_data, 0.8)\n",
    "    test       = list(dataset['test'])\n",
    "    return train, dev, test\n",
    "\n",
    "def _load_dbpedia():\n",
    "    dataset = load_dataset(\"dbpedia_14\")\n",
    "    print('loading dbpedia- hang tight')\n",
    "    train_data = dataset['train']\n",
    "    train_data = [_key_to_text(ex) for ex in tqdm(train_data)]\n",
    "    train, dev = _create_splits(train_data, 0.8)\n",
    "        \n",
    "    test  = dataset['test']\n",
    "    test = [_key_to_text(ex) for ex in test]\n",
    "    return train, dev, test\n",
    "\n",
    "def _create_splits(examples:list, ratio=0.8)->Tuple[list, list]:\n",
    "    examples = deepcopy(examples)\n",
    "    split_len = int(ratio*len(examples))\n",
    "    \n",
    "    random.seed(1)\n",
    "    random.shuffle(examples)\n",
    "    \n",
    "    split_1 = examples[:split_len]\n",
    "    split_2 = examples[split_len:]\n",
    "    return split_1, split_2\n",
    "\n",
    "def _key_to_text(ex:dict, old_key='content'):\n",
    "    \"\"\" convert key name from the old_key to 'text' \"\"\"\n",
    "    ex = ex.copy()\n",
    "    ex['text'] = ex.pop(old_key)\n",
    "    return ex\n",
    "\n",
    "\n",
    "    \n",
    "a, b, c = load_dbpedia()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2e00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0531225",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in tqdm(load_dataset(\"dbpedia_14\")['train']):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3c2c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "def bias_metric(x:np.ndarray, class_id):\n",
    "    return (x[class_id]/np.sum(x))\n",
    "    \n",
    "def sort_stats(stats, num_classes=2):\n",
    "    biased_words = {}\n",
    "    for class_id in range(num_classes):\n",
    "        class_words = sorted(stats.items(), key=lambda x: bias_metric(x[1], class_id), reverse=True)\n",
    "        biased_words[class_id] = class_words\n",
    "    return biased_words\n",
    "\n",
    "def get_biased_words(data_set, num_classes=2, single=False):\n",
    "    output = defaultdict(lambda: np.zeros(num_classes))\n",
    "    \n",
    "    for example in data_set:\n",
    "        label = example['label']\n",
    "        words = example['text'].split()\n",
    "        if single: words = set(words)\n",
    "        \n",
    "        for word in words:\n",
    "            output[word][label] += 1\n",
    "            \n",
    "    stats = sort_stats(output)\n",
    "    return stats\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train = list(dataset['train'])[:20]\n",
    "test  = list(dataset['test'])[:20]\n",
    "\n",
    "train_stats = get_biased_words(train, single=True)\n",
    "test_stats = get_biased_words(test, single=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bb05e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_stats[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e89d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2aaa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
