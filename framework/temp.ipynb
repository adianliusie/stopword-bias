{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c2d3501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2453, 2078, 1005, 1056], [2442, 2078, 1005, 1056], [2342, 2078, 1005, 1056], [2017, 1005, 2128], [2017, 1005, 2310], [2017, 1005, 2222], [2017, 1005, 1040], [2016, 1005, 1055], [2009, 1005, 1055], [2008, 1005, 2222], [2123, 1005, 1056], [2323, 1005, 2310], [4995, 1005, 1056], [2481, 1005, 1056], [2134, 1005, 1056], [2987, 1005, 1056], [2910, 1005, 1056], [8440, 1005, 1056], [4033, 1005, 1056], [3475, 1005, 1056], [17137, 1005, 1056], [5807, 1005, 1056], [2347, 1005, 1056], [4694, 1005, 1056], [2180, 1005, 1056], [2876, 1005, 1056], [2453, 2078], [2442, 2078], [2342, 2078], [1045], [2033], [2026], [2870], [2057], [2256], [14635], [9731], [2017], [2115], [6737], [4426], [25035], [2002], [2032], [2010], [2370], [2016], [2014], [5106], [2841], [2009], [2049], [2993], [2027], [2068], [2037], [17156], [3209], [2054], [2029], [2040], [3183], [2023], [2008], [2122], [2216], [2572], [2003], [2024], [2001], [2020], [2022], [2042], [2108], [2031], [2038], [2018], [2383], [2079], [2515], [2106], [2725], [1037], [2019], [1996], [1998], [2021], [2065], [2030], [2138], [2004], [2127], [2096], [1997], [2012], [2011], [2005], [2007], [2055], [2114], [2090], [2046], [2083], [2076], [2077], [2044], [2682], [2917], [2000], [2013], [2039], [2091], [1999], [2041], [2006], [2125], [2058], [2104], [2153], [2582], [2059], [2320], [2182], [2045], [2043], [2073], [2339], [2129], [2035], [2151], [2119], [2169], [2261], [2062], [2087], [2060], [2070], [2107], [2053], [4496], [2025], [2069], [2219], [2168], [2061], [2084], [2205], [2200], [1055], [1056], [2064], [2097], [2074], [2123], [2323], [2085], [1040], [2222], [1049], [1051], [2128], [2310], [1061], [7110], [4995], [2481], [2134], [2987], [2910], [8440], [4033], [3475], [5003], [17137], [5807], [2347], [4694], [2180], [2876]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "output = []\n",
    "for word in stopwords.words('english'):\n",
    "    ids = tokenizer(word).input_ids[1:-1]\n",
    "    output.append(ids)\n",
    "\n",
    "output.sort(key=lambda x: len(x), reverse=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1846b7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92e8ae60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 7592, 103, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "print(tokenizer('hello [MASK]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "740e6bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module src.trainer in src:\n",
      "\n",
      "NAME\n",
      "    src.trainer\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        Trainer\n",
      "    \n",
      "    class Trainer(builtins.object)\n",
      "     |  Trainer(exp_name: str, m_args: <function namedtuple at 0x7fa7ec2e5280>)\n",
      "     |  \n",
      "     |  \"base class for running basic transformer classification models\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, exp_name: str, m_args: <function namedtuple at 0x7fa7ec2e5280>)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  load_model(self, name: str = 'base')\n",
      "     |  \n",
      "     |  model_output(self, batch)\n",
      "     |  \n",
      "     |  save_model(self, name: str = 'base')\n",
      "     |  \n",
      "     |  set_up_helpers(self, m_args: <function namedtuple at 0x7fa7ec2e5280>)\n",
      "     |  \n",
      "     |  set_up_wandb(self, args: <function namedtuple at 0x7fa7ec2e5280>)\n",
      "     |  \n",
      "     |  system_eval = inner(*args, **kwargs)\n",
      "     |  \n",
      "     |  to(self, device)\n",
      "     |  \n",
      "     |  train(self, t_args: <function namedtuple at 0x7fa7ec2e5280>)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "DATA\n",
      "    List = typing.List\n",
      "        A generic version of list.\n",
      "    \n",
      "    Tuple = typing.Tuple\n",
      "        Tuple type; Tuple[X, Y] is the cross-product type of X and Y.\n",
      "        \n",
      "        Example: Tuple[T1, T2] is a tuple of two elements corresponding\n",
      "        to type variables T1 and T2.  Tuple[int, float, str] is a tuple\n",
      "        of an int, a float and a string.\n",
      "        \n",
      "        To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].\n",
      "\n",
      "FILE\n",
      "    /home/alta/Conversational/OET/al826/2022/shortcuts/new_system/src/trainer.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "235c218a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6ea682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.utils.data_utils import load_data\n",
    "from src.utils.torch_utils import load_tokenizer\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, trans_name):\n",
    "        self.tokenizer = load_tokenizer(trans_name)\n",
    "    \n",
    "    def prep_split(self, data:list):\n",
    "        output = []\n",
    "        for ex in tqdm(data):\n",
    "            text  = ex['text']\n",
    "            label = ex['label']\n",
    "            ids   = self.tokenizer(text).input_ids\n",
    "            output.append(SimpleNamespace(text=text, ids=ids, label=label))\n",
    "        return output\n",
    "    \n",
    "    def get_data(self, data_name:str):\n",
    "        print('tokenizing data')\n",
    "        train, dev, test = load_data(data_name)\n",
    "        train = self.prep_split(train)\n",
    "        dev   = self.prep_split(dev)\n",
    "        test  = self.prep_split(test)\n",
    "        return train, dev, test\n",
    "    \n",
    "D = DataLoader('bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5acde0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/alta/Conversational/OET/al826/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c5abaddc4b44349469d012e7fb9f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                        | 0/20000 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (727 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:25<00:00, 792.12it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:06<00:00, 819.47it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 25000/25000 [00:28<00:00, 874.41it/s]\n"
     ]
    }
   ],
   "source": [
    "train, dev, test = D.get_data('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945d8d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddded3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a0beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from tqdm import tqdm \n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Tuple\n",
    "from datasets import load_dataset\n",
    "\n",
    "def load_data(data_name:str)->Tuple['train', 'dev', 'test']:\n",
    "    if data_name == 'imdb':    return _load_imdb()\n",
    "    if data_name == 'dbpedia': return _load_dbpedia()\n",
    "    else: raise ValueError('invalid dataset provided')\n",
    "\n",
    "def _load_imdb()->List[Dict['text', 'label']]:\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "    train_data = list(dataset['train'])\n",
    "    train, dev = _create_splits(train_data, 0.8)\n",
    "    test       = list(dataset['test'])\n",
    "    return train, dev, test\n",
    "\n",
    "def _load_dbpedia():\n",
    "    dataset = load_dataset(\"dbpedia_14\")\n",
    "    print('loading dbpedia- hang tight')\n",
    "    train_data = dataset['train']\n",
    "    train_data = [_key_to_text(ex) for ex in tqdm(train_data)]\n",
    "    train, dev = _create_splits(train_data, 0.8)\n",
    "        \n",
    "    test  = dataset['test']\n",
    "    test = [_key_to_text(ex) for ex in test]\n",
    "    return train, dev, test\n",
    "\n",
    "def _create_splits(examples:list, ratio=0.8)->Tuple[list, list]:\n",
    "    examples = deepcopy(examples)\n",
    "    split_len = int(ratio*len(examples))\n",
    "    \n",
    "    random.seed(1)\n",
    "    random.shuffle(examples)\n",
    "    \n",
    "    split_1 = examples[:split_len]\n",
    "    split_2 = examples[split_len:]\n",
    "    return split_1, split_2\n",
    "\n",
    "def _key_to_text(ex:dict, old_key='content'):\n",
    "    \"\"\" convert key name from the old_key to 'text' \"\"\"\n",
    "    ex = ex.copy()\n",
    "    ex['text'] = ex.pop(old_key)\n",
    "    return ex\n",
    "\n",
    "\n",
    "    \n",
    "a, b, c = load_dbpedia()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2e00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0531225",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in tqdm(load_dataset(\"dbpedia_14\")['train']):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3c2c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "def bias_metric(x:np.ndarray, class_id):\n",
    "    return (x[class_id]/np.sum(x))\n",
    "    \n",
    "def sort_stats(stats, num_classes=2):\n",
    "    biased_words = {}\n",
    "    for class_id in range(num_classes):\n",
    "        class_words = sorted(stats.items(), key=lambda x: bias_metric(x[1], class_id), reverse=True)\n",
    "        biased_words[class_id] = class_words\n",
    "    return biased_words\n",
    "\n",
    "def get_biased_words(data_set, num_classes=2, single=False):\n",
    "    output = defaultdict(lambda: np.zeros(num_classes))\n",
    "    \n",
    "    for example in data_set:\n",
    "        label = example['label']\n",
    "        words = example['text'].split()\n",
    "        if single: words = set(words)\n",
    "        \n",
    "        for word in words:\n",
    "            output[word][label] += 1\n",
    "            \n",
    "    stats = sort_stats(output)\n",
    "    return stats\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train = list(dataset['train'])[:20]\n",
    "test  = list(dataset['test'])[:20]\n",
    "\n",
    "train_stats = get_biased_words(train, single=True)\n",
    "test_stats = get_biased_words(test, single=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bb05e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_stats[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e89d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2aaa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
